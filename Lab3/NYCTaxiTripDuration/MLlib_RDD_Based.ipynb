{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/08 12:22:38 WARN Utils: Your hostname, jztr resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/08 12:22:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/08 12:22:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/08 12:22:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/04/08 12:22:39 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import hour, minute, dayofweek, month, sqrt, pow\n",
    "\n",
    "from pyspark.mllib.linalg import DenseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .appName(\"NYCTaxiTripDurationRegression\")\\\n",
    "                    .master(\"local[*]\")\\\n",
    "                    .config(\"spark.log.level\", \"ERROR\")\\\n",
    "                    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "|       id|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|  pickup_longitude|   pickup_latitude| dropoff_longitude|  dropoff_latitude|store_and_fwd_flag|trip_duration|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "|id2875421|        2|2016-03-14 17:24:55|2016-03-14 17:32:30|              1| -73.9821548461914| 40.76793670654297|-73.96463012695312|40.765602111816406|                 N|          455|\n",
      "|id2377394|        1|2016-06-12 00:43:35|2016-06-12 00:54:38|              1|-73.98041534423828|40.738563537597656|-73.99948120117188| 40.73115158081055|                 N|          663|\n",
      "|id3858529|        2|2016-01-19 11:35:24|2016-01-19 12:10:48|              1| -73.9790267944336|40.763938903808594|-74.00533294677734|40.710086822509766|                 N|         2124|\n",
      "|id3504673|        2|2016-04-06 19:32:31|2016-04-06 19:39:40|              1|-74.01004028320312|   40.719970703125|-74.01226806640625| 40.70671844482422|                 N|          429|\n",
      "|id2181028|        2|2016-03-26 13:30:55|2016-03-26 13:38:10|              1|-73.97305297851562|40.793209075927734| -73.9729232788086| 40.78252029418945|                 N|          435|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"../../../data/train.csv\", header=True, inferSchema=True)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Columns**:\n",
    "  - `id`: Unique identifier for each trip.\n",
    "  - `vendor_id`: ID of the taxi vendor.\n",
    "  - `pickup_datetime` and `dropoff_datetime`: Timestamps for the start and end of the trip.\n",
    "  - `passenger_count`: Number of passengers in the taxi.\n",
    "  - `pickup_longitude` and `pickup_latitude`: GPS coordinates of the pickup location.\n",
    "  - `dropoff_longitude` and `dropoff_latitude`: GPS coordinates of the dropoff location.\n",
    "  - `store_and_fwd_flag`: Whether the trip record was held in the vehicle's memory before sending to the server (`Y` or `N`).\n",
    "  - `trip_duration`: Duration of the trip in seconds.\n",
    "\n",
    "1. **Feature Extraction**:\n",
    "  - Extracted additional features such as:\n",
    "    - `pickup_minutes`: Total minutes from the start of the day.\n",
    "    - `pickup_dayofweek`: Day of the week.\n",
    "    - `pickup_month`: Month of the year.\n",
    "    - `distance`: Euclidean distance between pickup and dropoff locations.\n",
    "\n",
    "2. **Filtering Invalid Data**:\n",
    "  - Removed trips with:\n",
    "    - `passenger_count` less than or equal to 0.\n",
    "    - `trip_duration` greater than 22 hours (extreme outliers).\n",
    "    - `distance` less than or equal to 0.\n",
    "\n",
    "3. **Feature Assembly**:\n",
    "  - Combined relevant features into a single vector using `VectorAssembler`. The selected features include:\n",
    "    - `passenger_count`\n",
    "    - `pickup_longitude`\n",
    "    - `pickup_latitude`\n",
    "    - `distance`\n",
    "    - `pickup_minutes`\n",
    "    - `pickup_dayofweek`\n",
    "    - `pickup_month`\n",
    "\n",
    "4. **Data Transformation**:\n",
    "  - Transformed the data into a format suitable for machine learning by creating a `features` column and retaining the target variable `trip_duration`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"pickup_minutes\", hour(\"pickup_datetime\") * 60 + minute(\"pickup_datetime\")) \\\n",
    "            .withColumn(\"pickup_dayofweek\", dayofweek(\"pickup_datetime\")) \\\n",
    "            .withColumn(\"pickup_month\", month(\"pickup_datetime\")) \\\n",
    "            .withColumn(\"distance\", sqrt(\n",
    "                pow(data[\"pickup_longitude\"] - data[\"dropoff_longitude\"], 2) +\n",
    "                pow(data[\"pickup_latitude\"] - data[\"dropoff_latitude\"], 2)\n",
    "            ))\n",
    "\n",
    "data = data.filter(\"passenger_count > 0\") \\\n",
    "            .filter(\"trip_duration < 22 * 3600\") \\\n",
    "            .filter(\"distance > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_columns = [\n",
    "    \"passenger_count\",\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"distance\",\n",
    "    \"pickup_minutes\",\n",
    "    \"pickup_dayofweek\",\n",
    "    \"pickup_month\",\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "assembled_data = assembler.transform(data)\n",
    "\n",
    "assembled_data = assembled_data.select(\"features\", \"trip_duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/08 12:22:50 WARN BlockManager: Task 18 already completed, not releasing lock for rdd_20_0\n",
      "[LabeledPoint(455.0, [1.0,-73.9821548461914,40.76793670654297,0.01767953949959892,1044.0,2.0,3.0])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert the features column  and target variable into RDD of LabeledPoint\n",
    "rdd_data = assembled_data.rdd.map(lambda row: LabeledPoint(row.trip_duration, DenseVector(row.features.values)))\n",
    "rdd_data.cache()\n",
    "\n",
    "print(rdd_data.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training:\n",
    "  - Split the data into training and testing sets.\n",
    "  - Train a Decision Tree Regressor using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/08 12:22:50 WARN BlockManager: Task 19 already completed, not releasing lock for rdd_20_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "train_rdd, test_rdd = rdd_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train a decision tree regressor model\n",
    "model = DecisionTree.trainRegressor(\n",
    "    train_rdd, \n",
    "    categoricalFeaturesInfo={},\n",
    "    maxDepth=10,\n",
    "    minInstancesPerNode=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation:\n",
    "  - Evaluate the model's performance on the test data using metrics such as RMSE and MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhthuy/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "[Stage 30:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 587.1664523389743\n",
      "Mean Absolute Error (MAE): 234.04290512445456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_rdd.map(lambda x: x.features))\n",
    "predictions_and_labels = predictions.zip(test_rdd.map(lambda lp: lp.label))\n",
    "\n",
    "# Use RegressionMetrics to evaluate the model\n",
    "metrics = RegressionMetrics(predictions_and_labels)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Root Mean Squared Error (RMSE):\", metrics.rootMeanSquaredError)\n",
    "print(\"Mean Absolute Error (MAE):\", metrics.meanAbsoluteError)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
