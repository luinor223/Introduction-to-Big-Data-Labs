{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 16:09:39 WARN Utils: Your hostname, lnvi-legion resolves to a loopback address: 127.0.1.1; using 192.168.100.212 instead (on interface wlp4s0)\n",
      "25/04/08 16:09:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/08 16:09:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from math import sqrt, pow\n",
    "from datetime import datetime\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "\n",
    "sc = SparkContext('local', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Change the path to the CSV file as needed\n",
    "# Load the CSV file as a text file and filter out the header\n",
    "lines = sc.textFile(\"train.csv\")\n",
    "header = lines.first()\n",
    "data_rdd = lines.filter(lambda line: line != header)\n",
    "\n",
    "def process_trip_line(line):\n",
    "    fields = line.split(\",\")\n",
    "    pickup_datetime = datetime.strptime(fields[2], \"%Y-%m-%d %H:%M:%S\") # pickup_datetime\n",
    "    passenger_count = int(fields[4]) # passenger_count\n",
    "    pickup_longitude = float(fields[5]) # pickup_longitude\n",
    "    pickup_latitude = float(fields[6]) # pickup_latitude\n",
    "    dropoff_longitude = float(fields[7]) # dropoff_longitude\n",
    "    dropoff_latitude = float(fields[8]) # dropoff_latitude\n",
    "    trip_duration = int(fields[10]) # trip_duration\n",
    "\n",
    "    pickup_minutes = pickup_datetime.hour * 60 + pickup_datetime.minute\n",
    "    pickup_dayofweek = pickup_datetime.weekday() + 1\n",
    "    pickup_month = pickup_datetime.month\n",
    "    distance = sqrt(pow((pickup_longitude - dropoff_longitude), 2) + pow((pickup_latitude - dropoff_latitude), 2))\n",
    "    return [\n",
    "        passenger_count,\n",
    "        pickup_latitude,\n",
    "        pickup_longitude,\n",
    "        distance,\n",
    "        pickup_minutes,\n",
    "        pickup_dayofweek,\n",
    "        pickup_month,\n",
    "        trip_duration\n",
    "    ]\n",
    "\n",
    "data_rdd = data_rdd.map(process_trip_line)\n",
    "data_rdd = data_rdd.filter(lambda x: x[0] > 0) \\\n",
    "                    .filter(lambda x: x[-1] < 22 * 3600) \\\n",
    "                    .filter(lambda x: x[3] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, train_rdd : RDD, max_depth=5, minInstancesPerNode=1, maxBins=10):\n",
    "        self.X = train_rdd.map(lambda row: row[:-1]).cache()\n",
    "        self.y = train_rdd.map(lambda row: row[-1]).cache()\n",
    "        self.max_depth = max_depth\n",
    "        self.minInstancesPerNode = minInstancesPerNode\n",
    "        self.maxBins = maxBins\n",
    "        self.tree = self.build_tree(self.X, self.y)\n",
    "    \n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.count(), len(X.first())\n",
    "        \n",
    "        y_mean = y.mean()\n",
    "        \n",
    "        variance = y.map(lambda x: (x - y_mean) ** 2).mean()\n",
    "        \n",
    "        if n_samples == 0 or depth >= self.max_depth or variance == 0:\n",
    "            return y_mean\n",
    "        \n",
    "        best_split = None\n",
    "        best_variance = float('inf')\n",
    "        \n",
    "        for feature_index in range(n_features):\n",
    "            split_candidates = self.approx_quantile(X.map(lambda x: x[feature_index]), [i / self.maxBins for i in range(1, self.maxBins)])\n",
    "            for candidate in split_candidates:\n",
    "                left_mask = X.map(lambda x: x[feature_index] <= candidate)\n",
    "                right_mask = X.map(lambda x: x[feature_index] > candidate)\n",
    "                \n",
    "                left_y = y.zip(left_mask).filter(lambda x: x[1]).map(lambda x: x[0]).persist()\n",
    "                right_y = y.zip(right_mask).filter(lambda x: not x[1]).map(lambda x: x[0]).persist()\n",
    "\n",
    "                if left_y.count() < self.minInstancesPerNode or right_y.count() < self.minInstancesPerNode:\n",
    "                    continue\n",
    "                \n",
    "                left_y_count = left_y.count()\n",
    "                right_y_count = right_y.count()\n",
    "                left_y_mean = left_y.mean()\n",
    "                right_y_mean = right_y.mean()\n",
    "\n",
    "                left_variance = left_y.map(lambda x: (x - left_y_mean) ** 2).mean() if left_y_count > 0 else 0\n",
    "                right_variance = right_y.map(lambda x: (x - right_y_mean) ** 2).mean() if right_y_count > 0 else 0\n",
    "                \n",
    "                weighted_variance = (left_variance * left_y_count + right_variance * right_y_count) / n_samples\n",
    "                \n",
    "                if weighted_variance < best_variance:\n",
    "                    best_variance = weighted_variance\n",
    "                    best_split = (feature_index, candidate)\n",
    "            left_y.unpersist()\n",
    "            right_y.unpersist()\n",
    "        \n",
    "        if best_split is None:\n",
    "            return y_mean\n",
    "        \n",
    "        feature_index, candidate = best_split\n",
    "        \n",
    "        left_mask = X.map(lambda x: x[feature_index] <= candidate)\n",
    "        right_mask = X.map(lambda x: x[feature_index] > candidate)\n",
    "        \n",
    "        left_X = X.zip(left_mask).filter(lambda x: x[1]).map(lambda x: x[0])\n",
    "        right_X = X.zip(right_mask).filter(lambda x: not x[1]).map(lambda x: x[0])\n",
    "        \n",
    "        left_y = y.zip(left_mask).filter(lambda x: x[1]).map(lambda x: x[0]).cache()\n",
    "        right_y = y.zip(right_mask).filter(lambda x: not x[1]).map(lambda x: x[0]).cache()\n",
    "        left_tree = self.build_tree(left_X, left_y, depth + 1)\n",
    "        right_tree = self.build_tree(right_X, right_y, depth + 1)\n",
    "        return {\n",
    "            'feature_index': feature_index,\n",
    "            'threshold': candidate,\n",
    "            'left': left_tree,\n",
    "            'right': right_tree\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def approx_quantile(rdd, quantiles, sample_fraction=0.1, seed=None):\n",
    "        \"\"\"\n",
    "        Approximate quantiles of an RDD using sampling.\n",
    "\n",
    "        :param rdd: Input RDD of numerical values.\n",
    "        :param quantiles: List of desired quantiles (values between 0 and 1).\n",
    "        :param sample_fraction: Fraction of data to sample (between 0 and 1).\n",
    "        :param seed: Random seed for sampling.\n",
    "        :return: List of approximate quantile values.\n",
    "        \"\"\"\n",
    "        sample = rdd.sample(withReplacement=False, fraction=sample_fraction, seed=seed).collect()\n",
    "\n",
    "        if not sample:\n",
    "            raise ValueError(\"Sample is empty. Consider increasing sample_fraction.\")\n",
    "\n",
    "        sample.sort()\n",
    "\n",
    "        n = len(sample)\n",
    "        return [sample[min(int(q * n), n - 1)] for q in quantiles]\n",
    "\n",
    "    def traverse_tree(self, X):\n",
    "        if isinstance(self.tree, dict):\n",
    "            if X[self.tree['feature_index']] <= self.tree['threshold']:\n",
    "                return self.traverse_tree(self.tree['left'], X)\n",
    "            else:\n",
    "                return self.traverse_tree(self.tree['right'], X)\n",
    "        else:\n",
    "            return self.tree\n",
    "\n",
    "    def predict(self, test_rdd):\n",
    "        return test_rdd.map(lambda X: self.traverse_tree(self.tree, X))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd, test_rdd = data_rdd.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                 (4 + 1) / 6]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "25/04/08 16:53:25 ERROR Executor: Exception in task 4.0 in stage 1828.0 (TID 10933)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 2343, in <lambda>\n",
      "    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(  # type: ignore[arg-type]\n",
      "                                         ^^^^^^^^^^^^^^\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 43, in __init__\n",
      "    self.merge(v)\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 52, in merge\n",
      "    self.minValue = minimum(self.minValue, value)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/04/08 16:53:25 WARN TaskSetManager: Lost task 4.0 in stage 1828.0 (TID 10933) (192.168.100.212 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 2343, in <lambda>\n",
      "    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(  # type: ignore[arg-type]\n",
      "                                         ^^^^^^^^^^^^^^\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 43, in __init__\n",
      "    self.merge(v)\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 52, in merge\n",
      "    self.minValue = minimum(self.minValue, value)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/04/08 16:53:25 ERROR TaskSetManager: Task 4 in stage 1828.0 failed 1 times; aborting job\n",
      "25/04/08 16:53:25 WARN TaskSetManager: Lost task 5.0 in stage 1828.0 (TID 10934) (192.168.100.212 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 1828.0 failed 1 times, most recent failure: Lost task 4.0 in stage 1828.0 (TID 10933) (192.168.100.212 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 2343, in <lambda>\n",
      "    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(  # type: ignore[arg-type]\n",
      "                                         ^^^^^^^^^^^^^^\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 43, in __init__\n",
      "    self.merge(v)\n",
      "  File \"/home/lnvi/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 52, in merge\n",
      "    self.minValue = minimum(self.minValue, value)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dt = \u001b[43mDecisionTreeRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_rdd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_rdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminInstancesPerNode\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mDecisionTreeRegressor.__init__\u001b[39m\u001b[34m(self, train_rdd, max_depth, minInstancesPerNode, maxBins)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mself\u001b[39m.minInstancesPerNode = minInstancesPerNode\n\u001b[32m      7\u001b[39m \u001b[38;5;28mself\u001b[39m.maxBins = maxBins\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mself\u001b[39m.tree = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mDecisionTreeRegressor.build_tree\u001b[39m\u001b[34m(self, X, y, depth)\u001b[39m\n\u001b[32m     41\u001b[39m right_y_mean = right_y.mean()\n\u001b[32m     43\u001b[39m left_variance = left_y.map(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x - left_y_mean) ** \u001b[32m2\u001b[39m).mean() \u001b[38;5;28;01mif\u001b[39;00m left_y_count > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m right_variance = \u001b[43mright_y\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_y_mean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m right_y_count > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     46\u001b[39m weighted_variance = (left_variance * left_y_count + right_variance * right_y_count) / n_samples\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weighted_variance < best_variance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/rdd.py:2523\u001b[39m, in \u001b[36mRDD.mean\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2501\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmean\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mRDD[NumberOrArray]\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m   2502\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2503\u001b[39m \u001b[33;03m    Compute the mean of this RDD's elements.\u001b[39;00m\n\u001b[32m   2504\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2521\u001b[39m \u001b[33;03m    2.0\u001b[39;00m\n\u001b[32m   2522\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2523\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstats\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/rdd.py:2343\u001b[39m, in \u001b[36mRDD.stats\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mredFunc\u001b[39m(left_counter: StatCounter, right_counter: StatCounter) -> StatCounter:\n\u001b[32m   2341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m left_counter.mergeStats(right_counter)\n\u001b[32m-> \u001b[39m\u001b[32m2343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mStatCounter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mredFunc\u001b[49m\n\u001b[32m   2345\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/rdd.py:1924\u001b[39m, in \u001b[36mRDD.reduce\u001b[39m\u001b[34m(self, f)\u001b[39m\n\u001b[32m   1921\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1922\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[32m-> \u001b[39m\u001b[32m1924\u001b[39m vals = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m vals:\n\u001b[32m   1926\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce(f, vals)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/pyspark/rdd.py:1833\u001b[39m, in \u001b[36mRDD.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1831\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m.context):\n\u001b[32m   1832\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1833\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonRDD\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1834\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m._jrdd_deserializer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/lab03-spark-ml/.venv/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         answer = smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:-\u001b[32m1\u001b[39m])\n\u001b[32m    512\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor(train_rdd=train_rdd, max_depth=5, minInstancesPerNode=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
